{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc03041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1\n",
    "# Simple Linear Regression uses one predictor (independent variable) to model the relationship with the response (dependent variable), producing a model with a single coefficient and intercept.\n",
    "# Multiple Linear Regression involves multiple predictors, allowing the model to capture more complex relationships and interactions among variables. The benefit of using multiple predictors is a potentially more accurate model, as it accounts for additional factors that can influence the response.\n",
    "\n",
    "# A continuous variable represents a range of values and is used directly in regression, creating a model that predicts changes in the response proportionally to the changes in this variable.\n",
    "# An indicator variable (or dummy variable) takes binary values (0 or 1) to represent categories. It shifts the intercept of the regression line depending on the category but doesn’t change the slope.\n",
    "# In Simple Linear Regression, using a continuous variable yields a line that adjusts based on the continuous predictor, while using an indicator variable creates a model that shifts up or down depending on the indicator's value.\n",
    "\n",
    "# Introducing an indicator variable in Multiple Linear Regression alongside a continuous predictor allows the model to have separate intercepts for each category (indicated by the indicator variable) but a shared slope for the continuous predictor.\n",
    "# In Simple Linear Regression, the model has a single line (intercept and slope) for all observations, but with Multiple Linear Regression that includes an indicator, there are two lines with different intercepts, reflecting category-based shifts in the response.\n",
    "\n",
    "# Adding an interaction term between a continuous and an indicator variable allows each category to have its own slope, meaning the model can capture different rates of change for each group.\n",
    "# This results in a Multiple Linear Regression model with distinct lines for each category that differ in both intercept and slope, providing a more flexible fit to the data based on category-specific trends.\n",
    "\n",
    "# When a model uses only indicator variables for a non-binary categorical predictor, it applies one-hot encoding to convert categories into binary variables (one for each category except a reference group).\n",
    "# This Multiple Linear Regression form models category-specific intercepts (with no slopes for continuous predictors). Each category has a separate expected outcome, and the response varies between categories but remains constant within each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46922f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2\n",
    "# step one: Identify the Predictor Values\n",
    "#   In the binary case, we classify each ad budget as either \"high\" or \"low,\" assigning 1 for high and 0 for low.\n",
    "# step two: Plug the Values into the Formula:\n",
    "# Without Interaction:\n",
    "#   Continuous:\n",
    "#     Sales=β0+β1⋅TV Spend+β2⋅Online Spend\n",
    "#     Sales=β0+β1⋅TV Spend+β 2⋅Online Spend\n",
    "# Insert the exact spending values for TV and online into the formula to predict sales based on their independent effects.\n",
    "#   Binary:\n",
    "#     Sales=β0+β1⋅TV High+β2⋅Online High\n",
    "#     Sales=β0+β1⋅TV High+β2⋅Online High\n",
    "# Use 1 or 0 for each variable, depending on whether the budget is high or low.\n",
    "\n",
    "# With Interaction Formula:\n",
    "# Continuous Case:\n",
    "#   Sales=β0+β1⋅TV Spend+β2⋅Online Spend+β3⋅(TV Spend×Online Spend)\n",
    "# Here, we still input the spending amounts, but we also calculate the interaction term(TV Spend×Online Spend), capturing the combined effect of both types of ad spending.\n",
    "# Binary Case:\n",
    "#   Sales=β0+β1⋅TV High+β2⋅Online High+β3⋅(TV High×Online High)\n",
    "# In this binary model, we input 1 or 0 for each of TV High and Online High and calculate the interaction term (TV High×Online High), which captures any combined effect when both budgets are high.\n",
    "\n",
    "# Without Interaction Model: Assumes that TV and online ad spending independently affect sales. We input each spending amount into the formula, adding their effects without any synergy. This model is suitable when the ads work independently of each other.\n",
    "# With Interaction Model: Includes an interaction term, capturing synergy between the two ad types. When we input spending values, the interaction term accounts for any extra effect when both TV and online ad spending are high. This model may provide more accurate predictions if one ad type enhances the effect of the other.\n",
    "# Difference: The interaction model reflects possible synergy between ad spending, offering more flexible predictions, while the non-interaction model assumes their effects are independent, providing a simpler prediction.\n",
    "\n",
    "# Without Interaction:\n",
    "#  Sales=β0+β1⋅TV High+β2⋅Online High\n",
    "# Here, TV High and Online High are binary indicators (1 for high spend, 0 for low). The coefficients β1 and β2capture the shift in sales when spending is high versus low for each advertising type.\n",
    "# With Interaction:\n",
    "#  Sales=β0+β1·TV High+β2⋅Online High+β3⋅(TV High×Online High)\n",
    "# Here, TV High×Online High is the interaction between the two binary indicators, representing cases where both ad budgets are high. The interaction term β3indicates the added effect on sales when both budgets are high, capturing any potential synergy between the two high-budget categories.\n",
    "# Making Predictions with Binary Variables:\n",
    "#   Without Interaction: Predict sales by inputting the binary values for high or low spending on each advertising medium.\n",
    "#   With Interaction: Use the interaction model to predict sales by considering not just whether spending is high or low but also if both spending levels are high together, potentially yielding different predictions that account for combined high-spending impacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3\n",
    "# For an additive model, we select a few predictor variables:\n",
    "#   Continuous Variable: Age\n",
    "#   Binary Variable: Gender (e.g., Male = 1, Female = 0)\n",
    "# Categorical Variable: Education Level (e.g., High School, Bachelor, Master)\n",
    "# The linear form for this additive model can be expressed as:\n",
    "#   Outcome=β0+β1⋅Age+β2⋅Gender+β3⋅Education Level\n",
    "# This model assumes that the effect of each predictor on the outcome is independent of the others.\n",
    "\n",
    "# For the interaction model, we incorporate an interaction term between age and gender. This allows us to examine if the relationship between age and the outcome differs based on gender.\n",
    "#   The linear form with the interaction term is:\n",
    "# Outcome=β0+β1⋅Age+β2⋅Gender+β3⋅Education Level+β4⋅(Age×Gender)\n",
    "# This model captures potential synergies between age and gender, reflecting if their combined effect is different from their individual effects.\n",
    "\n",
    "# Additive Model: Each predictor has an independent effect on the outcome. To predict using this model, input values for age, gender, and education level, then sum the effects according to the formula. This provides a straightforward estimate assuming no interaction between predictors.\n",
    "# Interaction Model: The interaction model includes an additional term for age and gender. When making predictions, input the values for age and gender and calculate the interaction term (Age × Gender). The interaction allows the effect of age on the outcome to vary based on gender, providing a more nuanced prediction.\n",
    "\n",
    "# Additive Model: Examine the p-values for each predictor. A significant p-value (typically <0.05) indicates that the predictor independently contributes to explaining the outcome. For example, if age and education level have low p-values, they have a statistically significant effect on the outcome.\n",
    "# Interaction Model: Besides the main effects, pay special attention to the interaction term's p-value. A significant interaction term suggests that the combined effect of age and gender is important for predicting the outcome. If not significant, it indicates that adding the interaction term may not provide additional insight.\n",
    "\n",
    "# Using Plotly, we create two scatter plots with best-fit lines to compare the additive and interaction models visually.\n",
    "# Additive Model Plot: This plot shows outcome predictions based on age and gender, without interaction. Each gender group will have a single line, implying the effect of age is consistent across genders.\n",
    "# Interaction Model Plot: This plot includes an interaction term, allowing different slopes for each gender. If the interaction is significant, we expect different trends between genders; otherwise, the lines may appear similar, suggesting the interaction term is unnecessary.\n",
    "# Comparing the plots:\n",
    "#   If the lines for each gender in the interaction plot differ significantly, this suggests the interaction term captures an important variation in the data, meaning the interaction model may be necessary.\n",
    "#   If the lines are similar in both plots, the additive model likely provides sufficient explanatory power, and the interaction term may be unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae39426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4\n",
    "# Model Fit (R-squared):\n",
    "#   When we say \"the model only explains 17.6% of the variability in the data,\" this refers to the R-squared value, a measure of how well the model as a whole captures the variability in the outcome. An R-squared of 17.6% indicates that 82.4% of the outcome's variability remains unexplained by the model, suggesting a weak fit overall.\n",
    "# Individual Coefficients and Significance:\n",
    "#   \"Many coefficients are larger than 10 with strong evidence against the null hypothesis\" indicates that specific predictors have substantial and statistically significant effects on the outcome. In other words, these variables are likely to influence the outcome individually, which is why their p-values are low (indicating strong evidence against the null hypothesis of \"no effect\").\n",
    "\n",
    "# Missing Variables: The model may omit important predictors, so while the included variables have significant effects, they only partially explain the outcome.\n",
    "# Complex Relationships: If interactions, non-linear relationships, or unmeasured external influences are present, a simple linear model with significant coefficients might still fail to capture the full variability in the data.\n",
    "# Thus, significant coefficients with a low R-squared suggest that while some predictors have strong effects, they are not enough on their own to fully explain the variability in the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d3d69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7\n",
    "# From model3/4 to model5:\n",
    "#   Rationale: Add more predictors to capture relevant features and explain more variance in the outcome.\n",
    "#   Principle: Improve model accuracy by including important characteristics while monitoring for multicollinearity.\n",
    "# From model5 to model6:\n",
    "#   Rationale: Simplify the model by retaining only statistically significant predictors.\n",
    "#   Principle: Focus on impactful predictors to reduce overfitting, making the model more interpretable and stable.\n",
    "# From model6 to model7:\n",
    "#   Rationale: Capture complex relationships by adding interaction terms and stabilize the model with centering and scaling.\n",
    "#   Principle: Enhance predictive power with interactions, while controlling multicollinearity to maintain generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241da4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8\n",
    "# # Re-running the full code to ensure proper execution and visualization\n",
    "\n",
    "# Re-importing necessary libraries in case of any session reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Simulating the dataset with relevant columns for the example\n",
    "n_samples = 100\n",
    "songs = pd.DataFrame({\n",
    "    'danceability': np.random.rand(n_samples),\n",
    "    'energy': np.random.rand(n_samples),\n",
    "    'loudness': np.random.rand(n_samples) * 100 - 50,  # ranging from -50 to +50\n",
    "    'mode': np.random.choice([0, 1], n_samples)  # binary variable\n",
    "})\n",
    "\n",
    "# Model formula for linear regression\n",
    "linear_form = 'danceability ~ energy * loudness + energy * mode'\n",
    "\n",
    "# Arrays to store R-squared values for in-sample and out-of-sample performance\n",
    "reps = 100  # Number of repetitions\n",
    "in_sample_Rsquared = np.zeros(reps)\n",
    "out_of_sample_Rsquared = np.zeros(reps)\n",
    "\n",
    "# Running the loop without a fixed random seed for train-test splitting\n",
    "for i in range(reps):\n",
    "    # Split the data (50-50 split)\n",
    "    songs_training_data, songs_testing_data = train_test_split(songs, train_size=0.5)\n",
    "    \n",
    "    # Fit the model on training data\n",
    "    final_model_fit = smf.ols(formula=linear_form, data=songs_training_data).fit()\n",
    "    \n",
    "    # Calculate in-sample R-squared\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    \n",
    "    # Calculate out-of-sample R-squared\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(\n",
    "        songs_testing_data['danceability'],\n",
    "        final_model_fit.predict(songs_testing_data)\n",
    "    )[0, 1]**2\n",
    "\n",
    "# Create a DataFrame to store the results for visualization\n",
    "df_results = pd.DataFrame({\n",
    "    \"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    "    \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "# Scatter plot of in-sample vs. out-of-sample R-squared values\n",
    "fig = px.scatter(\n",
    "    df_results, \n",
    "    x=\"In Sample Performance (Rsquared)\", \n",
    "    y=\"Out of Sample Performance (Rsquared)\", \n",
    "    title=\"In-Sample vs Out-of-Sample R-squared Performance Across 100 Splits\",\n",
    "    labels={\n",
    "        \"In Sample Performance (Rsquared)\": \"In-Sample R-squared\",\n",
    "        \"Out of Sample Performance (Rsquared)\": \"Out-of-Sample R-squared\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Adding a reference line for y=x to visualize agreement between in-sample and out-of-sample R-squared values\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\", name=\"y=x\", line_shape=\"linear\"))\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Loop for Repeated Splitting:\n",
    "#   The code splits the dataset into training and testing sets 100 times, without using a fixed random seed. This creates variation in how the data is divided each time, allowing us to observe different potential outcomes for model performance on unseen data.\n",
    "# In-Sample and Out-of-Sample R-squared Calculation:\n",
    "#   For each split, the model is trained on the training data, and both in-sample (training) and out-of-sample (testing) R-squared values are calculated.\n",
    "#   In-Sample R-squared measures the model’s ability to explain variance in the training data.\n",
    "#   Out-of-Sample R-squared assesses the model's generalization by evaluating its performance on the test data.\n",
    "# Visualization:\n",
    "#   The scatter plot of in-sample vs. out-of-sample R-squared values across all splits helps visualize the relationship between these metrics.\n",
    "#   A reference line y=x is added, which represents ideal agreement between in-sample and out-of-sample performance.\n",
    "\n",
    "# Close Agreement: If the points are close to the y=x line, it indicates that the model’s training performance closely matches its testing performance, suggesting a good generalization.\n",
    "# Deviation from Line: Points significantly above the line imply underfitting (both scores are low), while points below the line suggest overfitting (high training performance, low testing performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b315b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9\n",
    "# model6_fit is simpler, with fewer interactions and predictors, making it easier to understand and interpret.\n",
    "# model7_fit is more complex, with high-order interaction terms (e.g., four-way interactions). This added complexity aims to capture more detailed relationships in the data but makes the model harder to interpret.\n",
    "\n",
    "# Generalizability refers to the model’s ability to perform well on new, unseen data.\n",
    "# model7_fit may perform slightly better on random test data splits, suggesting it captures specific patterns in the training data.\n",
    "# However, this model can struggle when applied to data that comes from a different time or context (e.g., predicting future generations of data), which shows potential overfitting—relying too heavily on specific patterns from the training set.\n",
    "\n",
    "# model6_fit is more interpretable, meaning it’s easier to understand how each predictor affects the outcome, making it more useful when interpretability is essential.\n",
    "# model7_fit, despite sometimes showing better test performance, is harder to explain and may not always provide meaningful improvements in predictions for new data.\n",
    "\n",
    "# In real-world scenarios, data often arrives sequentially (e.g., over time or as new groups appear). This means models should ideally generalize well to data from new contexts.\n",
    "# The fact that model7_fit doesn’t perform as consistently when tested on future data indicates it may be less reliable for such sequential data, whereas the simpler model6_fit performs more steadily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://chatgpt.com/c/67366617-6c30-8013-9507-9460d6ae67b8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
